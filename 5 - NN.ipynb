{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6992c27d-3532-416d-bdd0-0f8433dc3276",
   "metadata": {},
   "source": [
    "<a></a>\n",
    "<div style=\"border-radius: 10px; border: 1px solid #0F9CF5; background-color: #232323; white-space: nowrap;\">\n",
    "    <p style=\"margin-top: -10px; margin-bottom: 0px; margin-left: 10px; font-size: 1.15em; padding: 10px; overflow: hidden;\">\n",
    "        <span style=\"color: orange; font-size: 2em;\">&#9432;  </span>\n",
    "        Click the <span style=\"color: orange;\">Run All</span> <img style=\"max-height: 1.5em; border: 1px solid orange;\" src=\"../img/RunAll.png\" /> button in the toolbar above to run the code in this notebook \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61409a09-7532-4307-81dd-8fe35e83f811",
   "metadata": {},
   "source": [
    "<a id=\"document-top\"></a>\n",
    "# BQuant Machine Learning Series Part 6 - Neural Networks\n",
    "\n",
    "<a href='https://bloombergslides.com/view/mail?iID=bfmm22RM8tDTtX2FRzRt'>Video: Episode 6 - ML Series Video - Neural Networks</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9acce60-08a7-415f-85e7-ad61d6bb88a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bql\n",
    "bq = bql.Service()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# cache bql request on disk\n",
    "import src.cache as cachereq\n",
    "from src.shared import * ## Shared library for retrieving data via BQL for Machine Learning Series\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87da162e-6eb6-4e43-b3eb-85b0f0c04a48",
   "metadata": {},
   "source": [
    "### Initial set up - PLEASE READ\n",
    "<font color='magenta'>The data is pre-cached on disk and will automatically be called when running get_earnings_factors_nn() function. The query sources significant amout of data from BQL so to avoid running into data limit issues, we strongly recommend you do not modify below code. You can examine BQL code in folder src -> shared.py\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c378d29d-a983-4861-a38b-a6a03371fbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = cachereq.CacheRequest(bq, {'cache_folder': 'data_neural_networks', 'cache_data_on_disk': True})\n",
    "\n",
    "# src -> shared.py -> get_earnings_factors()\n",
    "data = get_earnings_factors_nn(cache=cache)\n",
    "print(data.shape)\n",
    "print(\"Quarterly data: 2010-12-31 - 2020-12-31 for SP500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22cfd8c-8057-4cbb-a18b-85ae7f7bee53",
   "metadata": {},
   "source": [
    "<h3>Earnings movement prediction</h3>\n",
    "\n",
    "<h4>Forecast direction of next quarter earnings based on accounting information of the current quarter </h4>\n",
    "\n",
    "#### Steps:\n",
    "- Enhance data with additional information\n",
    "- Preprocess the data\n",
    "- Learn how to apply Neural Network algorithm on our dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b48d8-aa3b-4994-8765-4935d416784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25114fda-11ab-48e4-9357-5069993ecec9",
   "metadata": {},
   "source": [
    "#### Enhance data:\n",
    "- change in Earnings per share : (Current Period EPS - Prior Period EPS)\n",
    "- Assign 1 to positive change in EPS and 0 to negative change\n",
    "- Shift data index by -1: we will be using current financial data to predict future change in earnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e0235-23ea-450a-bfaa-c8b9b7d498b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary column of positive and negative earnings changes\n",
    "data['binary_change'] = [1 if row['change_in_EPS'] > 0 else 0 for _,row in data.iterrows()]\n",
    "\n",
    "# Shift date index by -1 so we are predicting future changes: 1 or 0\n",
    "data['Future_change'] = data['binary_change'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ec249b-9e4f-4a5a-ae32-17e7eef528ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal is to anticipate the sign of futute earnings change from the financial data of the current quarter.\n",
    "# If the future earnigs changes is + , we assign 1, otherwise 0,  to Future_change value of the current quarter\n",
    "data[['EPS','change_in_EPS','Future_change']].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75aaad2-52ca-4770-9d0f-97fdd002b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine data \n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9e256-1a55-414e-b0a8-b00b52214727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinity with nan\n",
    "data = data.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "#Drop rows where change_in_EPS is nan: they are no use to us \n",
    "data = data.dropna(subset = ['change_in_EPS', 'Future_change'])\n",
    "\n",
    "# We no longer need these columns\n",
    "data = data.drop(columns = ['EPS','change_in_EPS','binary_change'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb568a-2603-4d16-932a-5474f728feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine missing data\n",
    "missing_column_data = 100*(data.isnull().sum() / data.shape[0]).round(3)\n",
    "print('Percent of missing values per column:\\n', missing_column_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e566075b-9246-408b-aa97-0f18cfb79005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 10 columns that have more than 35% of data missing\n",
    "columns_to_drop = missing_column_data[missing_column_data > 35]\n",
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e54228-090e-4a89-90c9-4caffcf04d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of columns dropped, 10 \n",
    "data = data.drop(columns = list(columns_to_drop.index))\n",
    "print( f'New Dataframe shape : {data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f0625-490a-46e3-8165-fbb983f75c2c",
   "metadata": {},
   "source": [
    "#### Preprocess data:\n",
    "- Handle remaining missing values\n",
    "- Minimize influence of outliers by performing Winsorization\n",
    "- Standardize data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900f8498-9e42-49c3-88a9-a2a83fc83e05",
   "metadata": {},
   "source": [
    "Handle remaining missing data by replacing NaN by mean of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4a5d5b-71ab-450a-b3fe-f8f1e8679487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep in mind that this is a naive way to handle missing values. \n",
    "# This method can cause data leakage and does not factor the covariance between features.\n",
    "# For more robust methods,take a look at MICE or KNN\n",
    "\n",
    "for col in data.columns:\n",
    "    data[col].fillna(data[col].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ef45a-fb8c-4e6a-aa63-2ddebab24d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to split our data into train and test. \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Independent values/features\n",
    "X = data.iloc[:,:-1].values\n",
    "# Dependent values\n",
    "y = data.iloc[:,-1].values\n",
    "\n",
    "# Create test and train data sets, split data randomly into 20% test and 80% train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf36cad-5662-4a33-8c86-a3fbf24d144b",
   "metadata": {},
   "source": [
    "Winsorization transforms data by limiting extreme values, typically by setting all outliers to a specified percentile of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463fa45a-3383-47aa-bf96-a192372b623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mstats\n",
    "\n",
    "# Winsorize top 1% and bottom 1% of points\n",
    "# Apply on X_train and X_test separately\n",
    "X_train = mstats.winsorize(X_train, limits = [0.01, 0.01])\n",
    "X_test = mstats.winsorize(X_test, limits = [0.01, 0.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec1f39-f692-446c-9b61-4bde7b153872",
   "metadata": {},
   "source": [
    "Standardize the data\n",
    "\n",
    "$$z=(x-mean) /  Standard Deviation$$\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa1120d-f570-4bbd-85c5-096601b8b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: During testing, it is important to construct the test feature vectors using the means and standard deviations saved from\n",
    "# the training data, rather than computing it from the test data. You must scale your test inputs using the saved means\n",
    "# and standard deviations, prior to sending them to your Neural Networks library for classification.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit to training data and then transform it\n",
    "X_train = sc.fit_transform(X_train)\n",
    "# Perform standardization on testing data using mu and sigma from training data\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51306845-3a8f-48ba-b152-491a90ea6b68",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69233bde-6069-449a-ad2f-f23803b6c510",
   "metadata": {},
   "source": [
    "### Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks.\n",
    "\n",
    "* First it sums values of each input x multiplied by weight w\n",
    "* Weighted sum is passed through an activation function \n",
    "* Activation function \"converts\" output to binary output of 0 or 1\n",
    "* Weights are measure of influence that each input has on the final output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2704f9-5c2b-46ba-aa96-54fe7343fe76",
   "metadata": {},
   "source": [
    "<img src='img/perceptron.JPG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e710f0-d210-48f3-8c88-f63de0a9e856",
   "metadata": {},
   "source": [
    "### What is an Activation Function ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5e428b-82ba-41db-a0d6-7386ed993ba8",
   "metadata": {},
   "source": [
    "### Sigmoid function\n",
    "* Activation function has \"switch on\" and \"switch off\" characteristic\n",
    "* Moves from 0 to 1 depending on the input values of x\n",
    "* Activation function adds non-linearity to the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48e14e-655f-4d3c-be2d-461cbec387e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n",
    "# The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points.\n",
    "# There are four commonly used and popular activation functions â€” sigmoid, hyperbolic tangent(tanh), ReLU and Softmax.\n",
    "\n",
    "x = np.arange(-8, 8, 0.1)\n",
    "f = 1 / (1 + np.exp(-x))\n",
    "plt.plot(x, f)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Sigmoid function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7dffa-b865-4713-a73d-73684b714288",
   "metadata": {},
   "source": [
    "### Tanh function\n",
    "* Maps values between -1 and 1\n",
    "* tanh is also sigmoidal (s - shaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d83b1-8f8c-43f3-928e-3708bfd9d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-8, 8, 0.1)\n",
    "f = np.tanh(x)\n",
    "plt.plot(x, f)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Tanh function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372a2505-b325-4e0d-80c1-c17ceff0403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sigmoid function for later use\n",
    "# sigmoid(w*x + b) = 1/(1+e^-(wTx+b))\n",
    "# z is (w*x+b), \n",
    "\n",
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff00ca8a-3989-4b23-98cb-d12a9fa9a89d",
   "metadata": {},
   "source": [
    "##  Building blocks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ecfef-8b68-490d-ae24-711cf58a5329",
   "metadata": {},
   "source": [
    "### Structure of ANN\n",
    "<h5>Input Layer is where data enters the network</h5>  \n",
    "<h5>Hidden Layers (on the picture there are 2) is where function applies weights (w) to the inputs and directs them through activation function like sigmoid or relu</h5>  \n",
    "<h5>Output Layer is where function returns the outputs from the last layer</h5> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea4d2bd-5199-4a4a-9d06-90a76cc6b454",
   "metadata": {},
   "source": [
    "<img src='img/nn_structure.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf0b11e-1ddc-4326-a084-2e78534d6d9c",
   "metadata": {},
   "source": [
    "<h2>The general methodology to build a Neural Network is to:</h2>  \n",
    "\n",
    "1. Define the neural network structure ( # of input units,  # of hidden layers, etc). \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Implement forward propagation\n",
    "    - Compute loss\n",
    "    - Implement backward propagation to get the gradients\n",
    "    - Update parameters (gradient descent)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c2aa8b-f412-4d4d-b876-8725aae61430",
   "metadata": {},
   "source": [
    "<h4> 1 & 2 Define and Initialize model's parameters</h4> \n",
    "\n",
    "- n_x : size of the input layer\n",
    "- n_h : size of the hidden layer\n",
    "- n_y : size of the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5549ce8-6efc-49db-b3d5-269a1687df8f",
   "metadata": {},
   "source": [
    "Initialize weights (w) with random values and bias (b) as zeros.\n",
    "If we initialize weights with 0, the derivative with respect to a loss function will be the same for every w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fafde98-19d4-4190-837a-b49f41b2f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a basic network initialization\n",
    "\n",
    "# Size of the input layer\n",
    "n_x = 3\n",
    "# Size of the hidden layer\n",
    "n_h = 3\n",
    "# Size of the output layer\n",
    "n_y = 1\n",
    "\n",
    "\n",
    "# W1 - weight matrix of shape (n_h, n_x)\n",
    "W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "\n",
    "# b1 - bias vector of shape (n_h, 1)\n",
    "b1 = np.zeros((n_h,1))\n",
    "\n",
    "# W2 - weight matrix of shape (n_y, n_h)\n",
    "W2 = np.random.randn(n_y,n_h) * 0.01\n",
    "    \n",
    "# b2 - bias vector of shape (n_y, 1)\n",
    "b2 = np.zeros((n_y,1))\n",
    "\n",
    "print(\"W1 = \" + str(W1))\n",
    "print(\"b1 = \" + str(b1))\n",
    "print(\"W2 = \" + str(W2))\n",
    "print(\"b2 = \" + str(b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f2b1cb-b6f0-42ba-81a6-5f9beaa8f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build function to store parameters for later use\n",
    "\n",
    "def model_parameters(n_x, n_h, n_y): \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    # save to dictionary\n",
    "    parameters = {'W1' : W1,\n",
    "                  'b1' : b1,\n",
    "                  'W2' : W2,\n",
    "                  'b2' : b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb094193-61b1-4766-8164-26bbaf6f5333",
   "metadata": {},
   "source": [
    "### Forward propagation \n",
    "    \n",
    "* Calculations in the model that take us from an input layer all the way to the output ( how NN make predictions)\n",
    "* Each independent feature x will be passed to the 1st hidden layer combined with some randomized weight\n",
    "* 1st hidden layer applies an activation function resulting in an output which then becomes an input for next hidden layer\n",
    "* Next hidden layer, repeats step above and progresses forward\n",
    "* The weights of a neuron can me thought of as weights between 2 layers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f011dd2c-0421-4754-a2d4-a27d20ab74d7",
   "metadata": {},
   "source": [
    "<img src='img/forward_nn.JPG'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734fdc23-7056-4e52-9edb-3858cf8f3520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement forward pass \n",
    "# parameters - dictionary of initial parameters\n",
    "# X - input data\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Values from the picture above\n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    # use previously built function sigmoid\n",
    "    A2 = sigmoid(Z2)\n",
    "    # save to dictionary\n",
    "    fwd_pass_values = {\"Z1\" : Z1,\n",
    "                       \"A1\" : A1,\n",
    "                       \"Z2\" : Z2,\n",
    "                       \"A2\" : A2}\n",
    "    return A2, fwd_pass_values\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8a595-aca4-4840-8169-f415e0794319",
   "metadata": {},
   "source": [
    "Once the first forward pass has been completed and we have our prediction, how do we evaluate its accuracy? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd11b5-161f-4549-a333-dc786e46ba7d",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "    \n",
    "* It measures cost associated with an incorrect prediction\n",
    "* Our goal is to find coefficients that minimize the loss function\n",
    "* Cross entropy loss is used in classification problems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abde354-6f59-459e-8e69-0317e2f080e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement loss function\n",
    "# cost = -(1/m) * Sum(y*log(a^[2](i)) + (1-y)*log(1-a^[2](i)))\n",
    "# A2 - output of sigmoid \n",
    "# Y is a true output against which we'll be measuring the loss\n",
    "\n",
    "def entropy_loss(A2, Y, parameters):\n",
    "    m = Y.shape[1]\n",
    "    log_prob = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y)\n",
    "    cost = -(1 / m) * np.sum(log_prob)\n",
    "    # squeeze removes axes of length one from cost\n",
    "    cost = float(np.squeeze(cost))\n",
    "    return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14517939-406e-4e5c-8fa5-a964f44e3ef9",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "* Mechanism for tuning the weights based on the loss function\n",
    "* During training we want to find weights and biases that minimize the error (loss function)\n",
    "* To measure change in the loss function, we need to take the derivative of a function with respect to all the weights and biases.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66be7ca7-7e8a-4fe9-a2aa-c15e4190cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement function to measure derivatives\n",
    "# Pass dictionary of  parameters, forward propagation values, input data and labeled data\n",
    "\n",
    "def backward_propagation(parameters, fwd_pass_values, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    A1 = fwd_pass_values[\"A1\"]\n",
    "    A2 = fwd_pass_values[\"A2\"]\n",
    "    \n",
    "    # Derivatives of loss func w.r.t parameters\n",
    "    dZ2 = fwd_pass_values[\"A2\"] - Y\n",
    "    dW2 = 1 / m*np.dot(dZ2, fwd_pass_values[\"A1\"].T)\n",
    "    db2 = 1 / m*np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2)*(1 - np.power(A1, 2))\n",
    "    dW1 = 1 / m*np.dot(dZ1, X.T)\n",
    "    db1 = 1 / m*np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients =       {\"dW1\" : dW1,\n",
    "                       \"db1\" : db1,\n",
    "                       \"dW2\" : dW2,\n",
    "                       \"db2\" : db2}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711fe821-469b-4b59-9a4b-6a4d1c5f96d0",
   "metadata": {},
   "source": [
    "Now that we have derivatives, sensitivity of the loss function to change in parameters, how do we use them to update our weights and biases in order to decrease our loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee884f4e-8a85-4d21-b6ac-edd12e2df725",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "* Optimization algorithm used to find the values of parameters that minimize a cost function\n",
    "* We can use it to recursively update the weights by iterating over all training samples\n",
    "* It takes into account learning rate and initial parameter values\n",
    "* Learning rate controls size of the step on each iteration\n",
    "* parameter = parameter - learning rate * (derivative of loss function w.r.t parameter)\n",
    "* Derivative, slope of loss function, updates the change you want to make to the parameter \n",
    "* Ideally we want Gradient Descent convering to global optimum where derivative equals to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b5521-75c3-476e-833f-894a682504fe",
   "metadata": {},
   "source": [
    "<img src='img/gradient_nn.JPG'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2111aa09-fd90-4c70-b441-873335758568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters - dictionary with randomly initialized parameters \n",
    "# gradients - derivatives from backward_propagation function\n",
    "# parameter = parameter - learning rate * (derivative of loss function w.r.t parameter)\n",
    "\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate = 1.1):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    dW1 = gradients[\"dW1\"]\n",
    "    db1 = gradients[\"db1\"]\n",
    "    dW2 = gradients[\"dW2\"]\n",
    "    db2 = gradients[\"db2\"]\n",
    "    \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b591b4-af03-48d3-81c9-c70143a49594",
   "metadata": {},
   "source": [
    "<h2>Combine functions above and build your first neural network model</h2>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94975ee8-0551-4845-af5d-b1b8b8d56547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall our dataset\n",
    "\n",
    "print ('The shape of X_train: ' + str(X_train.shape))\n",
    "print ('The shape of y_train: ' + str(y_train.shape))\n",
    "print ('The shape of X_test: ' + str(X_test.shape))\n",
    "print ('The shape of y_test: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd376670-9043-4141-bc62-97bc98270bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data \n",
    "\n",
    "X_train_new = X_train.T\n",
    "y_train_new = y_train.reshape(1, y_train.shape[0])\n",
    "X_test_new = X_test.T\n",
    "y_test_new = y_test.reshape(1, y_test.shape[0])\n",
    "\n",
    "print ('The shape of X_train_new: ' + str(X_train_new.shape))\n",
    "print ('The shape of y_train_new: ' + str(y_train_new.shape))\n",
    "print ('The shape of X_test_new: ' + str(X_test_new.shape))\n",
    "print ('The shape of y_test_new: ' + str(y_test_new.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced761ac-d7f2-45d5-9c47-a2c2df171ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of input layer\n",
    "n_x = X_train_new.shape[0] # size of input layer\n",
    "# size of hidden layer\n",
    "n_h = 4\n",
    "# size of output layer\n",
    "n_y = y_train_new.shape[0]\n",
    "\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a56b61a-b7d0-412a-b0ac-09978d04efa7",
   "metadata": {},
   "source": [
    "<h3>Use model_parameter functions to initialize parameters</h3>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216da04a-d9a7-4819-bda0-5d54e9a848f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model_parameters(n_x, n_h, n_y)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cad8668-cf6b-4899-b9af-cd7344c640d1",
   "metadata": {},
   "source": [
    "<h3>Train Neural Network model</h3>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098cec8-c7ac-4c3e-9c9c-18829099571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations used in gradient descent for loop\n",
    "num_iterations = 10000\n",
    "\n",
    "for i in range(0, num_iterations):\n",
    "    \n",
    "    # Apply our forward propagation function\n",
    "    A2, fwd_pass_values = forward_propagation(X_train_new, parameters)\n",
    "    \n",
    "    # Calculate cost associated with an incorrect prediction\n",
    "    cost = entropy_loss(A2, y_train_new, parameters)\n",
    "    \n",
    "    # Apply backpropagation function to measure sensitivity of a loss function to parameters\n",
    "    gradients = backward_propagation(parameters, fwd_pass_values, X_train_new, y_train_new)\n",
    "    \n",
    "    # Update parameters using Gradient descent \n",
    "    parameters = update_parameters(parameters, gradients)\n",
    "    \n",
    "    # Print cost for every 1000th iteration\n",
    "    if i % 1000 == 0:\n",
    "        print(i,cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84aefb2-e73a-443f-8fd7-f1dde59fd306",
   "metadata": {},
   "source": [
    "<h3>Prediction</h3>  \n",
    "\n",
    "Now that we have our updated parameters that minimize the entropy loss, use forward propagation to make a prediction\n",
    "\n",
    "A2 is a vector of probabilities, recall it is a sigmoid()\n",
    "\n",
    "if A2 > 0.5 => 1, and 0 otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f23a29-ed14-49ff-8b9b-2f026567a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass test data into forward_propagation function along with newly optimized parameters\n",
    "A2, fwd_pass_values = forward_propagation(X_test_new, parameters)\n",
    "\n",
    "predictions = (A2 > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a174ab07-dccb-4dd6-847e-3ac2cb81f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "\n",
    "print ('Accuracy: %d' % float((np.dot(y_test_new , predictions.T) + np.dot(1 - y_test_new,1 - predictions.T))/float(y_test_new.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abe0bc-5745-4fb3-9fae-9454824e635b",
   "metadata": {},
   "source": [
    "<h3>Neural Networks with scikit-learn </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b445e9d0-fd40-48ef-9542-29fb4ba1f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#  Multi-layer Perceptron classifier contains one or more hidden layers and can learn non-linear functions. \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# hidden_layer_sizes allows us to set the number of layers and the number of nodes we wish to have in the Neural Network Classifier\n",
    "# max_iter denotes the number of epochs.\n",
    "# activation function for the hidden layers.\n",
    "# solver  specifies the algorithm for weight optimization across the nodes.\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes = (150,100,50), max_iter=300,activation = 'relu',solver = 'adam', random_state = 0)\n",
    "\n",
    "# Train\n",
    "mlp.fit(X_train,y_train)\n",
    "# Predict \n",
    "y_pred = mlp.predict(X_test)\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e297e0-4526-4d00-ac75-66056a6f257d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (sandboxed)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
