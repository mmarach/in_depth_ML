{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a></a>\n",
    "<div style=\"border-radius: 10px; border: 1px solid #0F9CF5; background-color: #232323; white-space: nowrap;\">\n",
    "    <p style=\"margin-top: -10px; margin-bottom: 0px; margin-left: 10px; font-size: 1.15em; padding: 10px; overflow: hidden;\">\n",
    "        <span style=\"color: orange; font-size: 2em;\">&#9432;  </span>\n",
    "        Click the <span style=\"color: orange;\">Run All</span> <img style=\"max-height: 1.5em; border: 1px solid orange;\" src=\"../img/RunAll.png\" /> button in the toolbar above to run the code in this notebook \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"document-top\"></a>\n",
    "# BQuant Machine Learning Series Part 4 - Decision Trees and Random Forests\n",
    "\n",
    "\n",
    "<a href='https://bloombergslides.com/view/mail?iID=PhKMSzF7kdZqGgqVgq8q'>Link to Episode 4 - ML Series Video - Decision Trees and Random Forests</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bql\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bqviz as bqv\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from datetime import date\n",
    "\n",
    "# cache bql request on disk\n",
    "import src.cache as cachereq\n",
    "from src.shared import * ## Shared library for retrieving data via BQL for Machine Learning Series\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers and regressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# scoring the model\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# encode target labels with value between 0 and n_classes-1.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# visualize a tree\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Theory\n",
    "Decision Trees are a form of supervised ML that seek to build a simple set of decision rules to make predictions.\n",
    "DT are among the most popular machine learning algorithms given their interpretability and simplicity.\n",
    "* DT can be applied for both classification (the predicted outcome is the class or category) and regression (the predicted outcome is continuous number), but mainly used for classifications\n",
    "* DT is a foundation of Random Forests, which uses ensemble of different DTs and corrects overfitting\n",
    "* DT are attractive because of interpretability and simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/decision_tree.png\" width=\"80%\" height=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/decision_boundary.png\" width=\"60%\" height=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source: scikit-learn](https://scikit-learn.org/stable/modules/tree.html) <br>\n",
    "**Advantages:**\n",
    "* simple to understand and to interpret. Trees can be visualized.\n",
    "* requires little data preparation\n",
    "* able to handle both numerical and categorical data\n",
    "* uses a white box model ~ results is easy to interpret\n",
    "* the cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree\n",
    "* able to handle multi-output problems\n",
    "* possible to validate a model using statistical tests\n",
    "* performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "\n",
    "**Disadvantages:**\n",
    "* decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting. \n",
    "* DTs can be unstable ~ resolve by using an ensemble of DT\n",
    "* cannot guarantee to return the globally optimal decision tree\n",
    "* predictions of DTs are neither smooth nor continuous, but piecewise constant approximations as seen in the above figure. Therefore, they are not good at extrapolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some mechanism to avoid overfitting:** \n",
    "* pruning or reducing the size of DT by removing sections of the tree that are non-critical and redundant to classify instances\n",
    "* setting the minimum number of samples required at a leaf node \n",
    "* setting the maximum depth of the tree\n",
    "* creating ensemble of DTs ~ Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An algorithm calculates  numerical measure to decide which feature to split and at which threshold at each step in building the tree.<br>\n",
    "These generally measure the homogeneity of the target variable within the subsets.\n",
    "* Gini impurity (different from gini coefficient) is a measure of purity or variability of categorical data\n",
    "$$Gini=1-\\sum^r_j p^2_j$$\n",
    "* for example if 30% of sample was classified as CLASS1 and 70% as CLASS2, Gini impurity = 1 - (0.3 * 0.3 + 0.7 * 0.7 ) = 0.42\n",
    "* entropy is the measure of disorder of a variable\n",
    "$${\\displaystyle \\mathrm {Entropy}(X)=-\\sum _{i=1}^{n}{\\mathrm {P} (x_{i})\\log _{b}\\mathrm {P} (x_{i})},}$$\n",
    "* for the same example entropy = - 0.3 * log2(0.3) - 0.7 * log2(0.7) = 0.88\n",
    "* the goal is to find a split that best reduces the entropy\n",
    "* Gini impurity and Entropy tend to generate similar result \\ tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial set up\n",
    "<font color='magenta'>The data for given parameters is pre-cached on disk and automatically will be fetched as far as the query stays the same.\n",
    "Please do not change parameters (and as a result  change the query) to avoid BQL run. Query may sources significant amount of data and can lead to data limit issues. At the same time you have an access to BQL query and can study and run it as needed.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the dates if you want to use pre-cached data\n",
    "start_date  = date(2017, 10, 1)\n",
    "periods     = 9 # source 9 quarters\n",
    "dates = pd.date_range(start_date, periods=periods, freq='QS')\n",
    "dates = [ x.date() for x in dates ]\n",
    "bq          = bql.Service()\n",
    "\n",
    "# we will use this object to automatically cache the data on disk or read pre-saved data\n",
    "cache = cachereq.CacheRequest(bq, {'cache_folder': 'data', 'cache_data_on_disk': True})\n",
    "\n",
    "print(f'We will source the data for the following dates : {[ str(x) for x in dates ]}')\n",
    "# dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Bond Data\n",
    "- start with only one date to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_of_date = dates[0]\n",
    "\n",
    "df = get_bond_data(as_of_date, cache=cache)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source data for all dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for as_of_date in dates:\n",
    "    df = get_bond_data(as_of_date, cache=cache)\n",
    "    data.append( df )\n",
    "    \n",
    "data = pd.concat(data)\n",
    "data_orig = data.copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriching the data with extra information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding rating at the next period as we will predict the FUTURE and not the current rating\n",
    "- this approach would make the model design cleaner, but not strictly necessary as only 6% of securities change the rating\n",
    "- about 3.5% securities disappear from the rating from one period to another\n",
    "- finally we will predict rating category (high yield, investment grade etc.) and even fewer securities will change the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# prepare the dict with security + date key and rating as a value\n",
    "current_date_rating = dict( zip(data.index + '_' + data['As_of_date'].astype(str), data['Rating']) )\n",
    "\n",
    "# create a column with next period\\quarter rating (future rating)\n",
    "# tech details: find in prepared current_date_rating a key, which is equal security + date+3months\n",
    "rating_future = data[['As_of_date']].reset_index().apply( lambda x : current_date_rating.get(x[0] + '_' + str(x[1] + relativedelta(months=+3)), ''), axis = 1)\n",
    "data.insert(data.columns.tolist().index('Rating') + 1, 'Rating_future', rating_future.values)\n",
    "\n",
    "# remove the last date and the future rating is not known for this set by definition\n",
    "data = data[~(data['As_of_date'] == dates[-1])]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the current rating distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame( data['Rating_future'].value_counts() ).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remapping rating to the category (high yield, investment grade etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove distressed and defaulted bonds ~ 3.5% of population \n",
    "# as well as ones with empty future rating ~ 3% (when exclude the last date for which the future rating is null by definition)\n",
    "\n",
    "data = data[ ~data['Rating_future'].isin(['CCC+', 'CCC', 'CCC-', 'CC+', 'CC', 'CC-', 'C+', 'C', 'C-', 'DD+', 'DDD+', ''])]\n",
    "\n",
    "# remapping the rating to Investment Grade High, Investment Grade Low and High Yield categories\n",
    "rating_map = { 'AAA' : 'IG High', 'AA+' : 'IG High', 'AA' : 'IG High', 'AA-' : 'IG High', 'A+' : 'IG High', 'A' : 'IG High', 'A-' : 'IG High',\n",
    "               'BBB+' : 'IG Low', 'BBB' : 'IG Low', 'BBB-' : 'IG Low', \n",
    "               'BB+' : 'HY', 'BB' : 'HY', 'BB-' : 'HY', 'B+' : 'HY', 'B' : 'HY', 'B-' : 'HY'  }\n",
    "\n",
    "data.insert(data.columns.tolist().index('Rating_future') + 1, 'Category_future', data['Rating_future'].apply( lambda x : rating_map[x] ))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check count of the predicted category\n",
    "- we have three approximately equal categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display( pd.DataFrame( data['Category_future'].value_counts() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( f'Dataframe shape={data.shape}' )\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "- with decision trees an impact of  outliers in in predictor variables (not target variables) are small.\n",
    "- so we skip normalization step (winsorization, z-score etc.) as it is not necessary and one of the model advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_display = data[data['As_of_date'] == dates[-2]] #check only one date for performance issues\n",
    "data_to_display = data_to_display.drop(columns = ['As_of_date', 'Rating_future', 'Issuer', 'Industry', 'Rating'])\n",
    "\n",
    "plot = bqv.InteractiveScatterPlot(data_to_display,  color_field='Category_future',  reg_line=False)\n",
    "plot.y_control.value = 'Spread_OAS'\n",
    "plot.x_control.value = 'Duration'\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use two features to predict Bond Category\n",
    "1. Define features and labels\n",
    "2. Split data into train and test\n",
    "3. Train the model\n",
    "4. Make a prediction\n",
    "5. Check the probabilities\n",
    "6. Check the feature importance\n",
    "7. Check the model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features or independent variables\n",
    "features = ['Spread_OAS', 'Duration']\n",
    "# Our target or dependent variable\n",
    "target = 'Category_future'\n",
    "\n",
    "# set the first six quarters as the train set\n",
    "data_dropna = data.dropna(subset=features)\n",
    "X_train = data_dropna.loc[ ~data_dropna['As_of_date'].isin(dates[-3:]), features]\n",
    "y_train = data_dropna.loc[ ~data_dropna['As_of_date'].isin(dates[-3:]), target]\n",
    "\n",
    "# set the last six quarters as the test set\n",
    "X_test  = data_dropna.loc[ data_dropna['As_of_date'].isin(dates[-3:]), features]\n",
    "y_test  = data_dropna.loc[ data_dropna['As_of_date'].isin(dates[-3:]), target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape = \",  X_train.shape, \"   y_train shape = \", y_train.shape[0])\n",
    "print(\"X_test shape  = \",  X_test.shape,  \"   y_test shape  = \", y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model and get out-of-sample prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classifier = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=63)\n",
    "# tree_classifier = DecisionTreeClassifier(criterion='gini', random_state=63)\n",
    "# fit the model on training data\n",
    "tree_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "prediction = tree_classifier.predict( X_test )\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check probabilities,\n",
    "# for example first raw represents probability of the class belonging to class 'HY' or tree_classifier.classes_[0]\n",
    "print( f'Classes names: {tree_classifier.classes_}' )\n",
    "\n",
    "probabilities = tree_classifier.predict_proba(X_test)\n",
    "\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distr_of_prob(classifier, probabilities):\n",
    "    plt.figure(figsize=(10,5))\n",
    "\n",
    "    for i, k in enumerate(classifier.classes_):\n",
    "        sns.kdeplot(probabilities[:,i], shade=True, label = k)\n",
    "\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.title('Distribution of Probabilities by class', fontsize=14)\n",
    "    plt.show()\n",
    "    \n",
    "plot_distr_of_prob(tree_classifier, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "feature_importances = pd.DataFrame({'Feature': X_train.columns.tolist(),\n",
    "                                    'Importance': tree_classifier.feature_importances_.round(3)}).\\\n",
    "                                    sort_values('Importance', ascending = False)\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model score\n",
    "def print_score(classifier, X, y):\n",
    "    # accuracy classification score\n",
    "    print('Accuracy Score: {0:.4f}'.format( accuracy_score(y, classifier.predict(X)) ))\n",
    "        \n",
    "    # Precision is the ratio of true positives to total predicted positives (=TP / (TP + FP))\n",
    "    # Recall is the ratio of true positives to total actual positives (=TP / (TP + FN))\n",
    "    # F1-score provides a single score that balances both the concerns of precision and recall in one number\n",
    "    print('Classification Report: \\n {}\\n'.format( classification_report(y, classifier.predict(X)) ))\n",
    "    \n",
    "    # CM summarizes the performance of a classification algorithm\n",
    "    # we have true positives count on the diagonal and false negative elsewhere\n",
    "    print('Confusion Matrix: \\n {}\\n'.format( confusion_matrix(y, classifier.predict(X)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model score\n",
    "print('************ Train set ************')\n",
    "print_score(tree_classifier, X_train, y_train)\n",
    "\n",
    "print('************ Test set ************')\n",
    "print_score(tree_classifier, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to plot decision regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, classifier, resolution=0.1, xlabel=None, ylabel=None, title=None):\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap( colors[:len(np.unique(y))] )\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "\n",
    "    #encoding label to numerical value for plotting in case it is not numerical already\n",
    "    if type( Z[0] ) == str:\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(y)\n",
    "        Z = label_encoder.transform( Z )\n",
    "\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "\n",
    "    plt.figure(figsize=[6,4], dpi=100)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8, \n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx],\n",
    "                    label=cl,\n",
    "                    edgecolor='black')\n",
    "\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "#     plt.savefig('img/decision_boundary.png', format='png', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting decision boundary of a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(X_train.values, y_train.values, tree_classifier, resolution=0.1, xlabel=X_train.columns[0], ylabel=X_train.columns[1], title = 'Decision Boundary, Train Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting decision boundary applied to the out-of-sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(X_test.values, y_test.values, tree_classifier, resolution=0.1, xlabel=X_test.columns[0], ylabel=X_test.columns[1], title = 'Decision Boundary, Test Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,4], dpi=600)\n",
    "# plot_tree(tree_classifier, feature_names = feature_names, class_names =class_names, filled=True)\n",
    "feature_names = [x[0:10] for x in X_train.columns.tolist()]\n",
    "plot_tree(tree_classifier, filled=True, feature_names = feature_names, class_names= np.unique( y_train.values ) )\n",
    "# plt.savefig('img/decision_tree.png', format='png', dpi=600)\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg = data_orig.copy()\n",
    "\n",
    "# prepare the dict with security + date key and rating as a value\n",
    "current_date_spread = dict( zip(data_reg.index + '_' + data_reg['As_of_date'].astype(str), data_reg['Spread_OAS']) )\n",
    "\n",
    "# create a column with next period\\quarter spread (future spread)\n",
    "# tech details: find in prepared current_date_spread a key, which is equal security + date+3months\n",
    "spread_future = data_reg[['As_of_date']].reset_index().apply( lambda x : current_date_spread.get(x[0] + '_' + str(x[1] + relativedelta(months=+3)), np.nan), axis = 1)\n",
    "data_reg.insert(data_reg.columns.tolist().index('Spread_OAS') + 1, 'Spread_OAS_future', spread_future.values)\n",
    "\n",
    "data_reg = data_reg.dropna(subset=['Spread_OAS', 'Spread_OAS_future'])\n",
    "data_reg = data_reg[(data_reg['Spread_OAS'] >0) & (data_reg['Spread_OAS_future'] >0) ]\n",
    "\n",
    "data_reg.insert(data_reg.columns.tolist().index('Spread_OAS_future') + 1, 'Spread_OAS_future_change', data_reg['Spread_OAS_future'] / data_reg['Spread_OAS'] -1  )\n",
    "\n",
    "data_reg['Spread_OAS_future_change'] = data_reg['Spread_OAS_future_change'].clip(upper=3)\n",
    "\n",
    "data_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.kdeplot(data_reg['Spread_OAS_future_change'], shade=True)\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.title('Distribution of OAS Spread change', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features or independent variables\n",
    "features = ['Spread_OAS', 'Duration']\n",
    "# Our target or dependent variable\n",
    "target = 'Spread_OAS_future_change'\n",
    "\n",
    "# set the first six quarters as the train set\n",
    "data_dropna = data_reg.dropna(subset=features)\n",
    "X_train = data_dropna.loc[ ~data_dropna['As_of_date'].isin(dates[-3:]), features]\n",
    "y_train = data_dropna.loc[ ~data_dropna['As_of_date'].isin(dates[-3:]), target]\n",
    "\n",
    "# set the last six quarters as the test set\n",
    "X_test  = data_dropna.loc[ data_dropna['As_of_date'].isin(dates[-3:]), features]\n",
    "y_test  = data_dropna.loc[ data_dropna['As_of_date'].isin(dates[-3:]), target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_regressor = DecisionTreeRegressor(max_depth=3, random_state=63)\n",
    "# fit the model on training data\n",
    "tree_regressor.fit(X_train, y_train)\n",
    "\n",
    "# make a prediction\n",
    "prediction = tree_regressor.predict( X_test )\n",
    "print('DecisionTreeRegressor prediction:', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Techniques\n",
    "* Ensemble is about joining different predictors into one.\n",
    "\n",
    "There are several ensemble techniques, among them\n",
    "* **bagging** uses sampling with replacement, also called bootstrapping\n",
    "    * combine regression with average and classification with voting\n",
    "    * running a bunch of models in a parallel way as each model is trained by a random subset of the data\n",
    "* **pasting** uses sampling without replacement\n",
    "* **boosting** - first we train week classifiers and then add them to strong classifier by weighting them by accuracy\n",
    "    * training a bunch of individual models in a sequential way as each individual model learns from mistakes made by the previous model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Theory\n",
    "* Random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction. Random forest correct for decision trees' habit of overfitting to their training set\n",
    "* random forest is trained by bagging method with random sampling of observations and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "1. Define features and labels\n",
    "2. Split data into train and test\n",
    "3. Train the model\n",
    "4. Make a prediction\n",
    "5. Check the probabilities\n",
    "6. Check the feature importance\n",
    "7. Check the model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features or independent variables\n",
    "features = [ 'Spread_OAS', 'Prob_of_default', 'Yield', 'Coupon', 'Amt_Outst', 'Duration', 'Maturity_Years', 'EBITDA_Margin', 'Return_on_Assets', \n",
    "            'Current_Ratio', 'Quick_Ratio', 'Debt_to_EBITDA', 'Debt_to_Assets', 'Debt_to_Equity', 'Return_on_Equity_3yr_avg']\n",
    "\n",
    "# Our target or dependent variable\n",
    "target = 'Category_future'\n",
    "\n",
    "# set the first six quarters as the train set\n",
    "data_dropna = data.dropna(subset=features)\n",
    "X_train = data_dropna.loc[ ~data_dropna['As_of_date'].isin(dates[-3:]), features]\n",
    "y_train = data_dropna.loc[ ~data_dropna['As_of_date'].isin(dates[-3:]), target]\n",
    "\n",
    "# set the last six quarters as the test set\n",
    "X_test  = data_dropna.loc[ data_dropna['As_of_date'].isin(dates[-3:]), features]\n",
    "y_test  = data_dropna.loc[ data_dropna['As_of_date'].isin(dates[-3:]), target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape = \",  X_train.shape, \"   y_train shape = \", y_train.shape[0])\n",
    "print(\"X_test shape  = \",  X_test.shape,  \"   y_test shape  = \", y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators - the number of trees in the forest.\n",
    "# max_depth - the maximum depth of the tree\n",
    "# bootstrap - whether bootstrap samples are used when building trees and the samples are drawn with replacement\n",
    "rf_classifier = RandomForestClassifier(n_estimators=1000, bootstrap = True, max_depth = 6, random_state = 63)\n",
    "\n",
    "# fit the model on training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# make a prediction\n",
    "prediction = rf_classifier.predict(X_test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check probabilities,\n",
    "# for example first raw represents probability of the class belonging to class 'HY' or tree_classifier.classes_[0]\n",
    "print( f'Classes names: {rf_classifier.classes_}' )\n",
    "probabilities = rf_classifier.predict_proba(X_test)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distr_of_prob(rf_classifier, probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "- it is very easy to measure the relative importance of each feature on the prediction with Random Forest\n",
    "- achieved by looking at how much the tree nodes that use that feature reduce impurity across all trees in the forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame({'Feature': X_train.columns.tolist(),\n",
    "                                    'Importance': rf_classifier.feature_importances_.round(3)}).\\\n",
    "                                    sort_values('Importance', ascending = False)\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model score\n",
    "print('************ Train set ************')\n",
    "print_score(rf_classifier, X_train, y_train)\n",
    "\n",
    "print('************ Test set ************')\n",
    "print_score(rf_classifier, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (sandboxed)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
