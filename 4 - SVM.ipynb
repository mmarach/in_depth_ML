{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a></a>\n",
    "<div style=\"border-radius: 10px; border: 1px solid #0F9CF5; background-color: #232323; white-space: nowrap;\">\n",
    "    <p style=\"margin-top: -10px; margin-bottom: 0px; margin-left: 10px; font-size: 1.15em; padding: 10px; overflow: hidden;\">\n",
    "        <span style=\"color: orange; font-size: 2em;\">&#9432;  </span>\n",
    "        Click the <span style=\"color: orange;\">Run All</span> <img style=\"max-height: 1.5em; border: 1px solid orange;\" src=\"../img/RunAll.png\" /> button in the toolbar above to run the code in this notebook \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"document-top\"></a>\n",
    "# BQuant Machine Learning Series Part 5 - SVM\n",
    "\n",
    "\n",
    "\n",
    "<a href='https://bloombergslides.com/view/new/mail?iID=4kX986qfQH4VFvvbWG77'>Video: Episode 5 - ML Series Video - Support Vector Machines</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bql\n",
    "bq = bql.Service()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# cache bql request on disk\n",
    "import src.cache as cachereq\n",
    "from src.shared import * ## Shared library for retrieving data via BQL for Machine Learning Series\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial set up - PLEASE READ\n",
    "<font color='magenta'>The data is pre-cached on disk and will automatically be called when running get_earnings_factors() function. The query sources significant amout of data from BQL so to avoid running into data limit issues, we strongly recommend you do not modify below code. You can examine BQL code in folder src -> shared.py\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cached data from data_svm folder\n",
    "cache = cachereq.CacheRequest(bq, {'cache_folder': 'data_svm', 'cache_data_on_disk': True})\n",
    "\n",
    "# src -> shared.py -> get_earnings_factors()\n",
    "data = get_earnings_factors(cache=cache)\n",
    "print(data.shape)\n",
    "print(\"Quarterly data from 2018-12-31 : 2020-12-31 for SP500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Earnings movement prediction</h3>\n",
    "\n",
    "<h4>Forecast direction of next quarter earnings based on accounting information of the current quarter </h4>\n",
    "\n",
    "#### Steps:\n",
    "- Enhance data with additional information\n",
    "- Preprocess the data\n",
    "- Apply Support Vector Machines on our dataset\n",
    "- Try to improve our results through PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enhance data:\n",
    "- change in Earnings per share : (Current Period EPS - Prior Period EPS) \n",
    "- Assign 1 to positive change in EPS and 0 to negative change\n",
    "- Shift data index by -1: we will be using current financial data to predict future change in earnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary column of positive and negative earnings changes\n",
    "data['binary_change'] = [1 if row['change_in_EPS'] > 0 else 0 for _,row in data.iterrows()]\n",
    "\n",
    "# Shift date index by -1 so we are predicting future changes: 1 or 0\n",
    "data['Future_change'] = data['binary_change'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal is to anticipate the sign of futute earnings change from the financial data of the current quarter.\n",
    "# If the future earnigs changes is + , we assign 1, otherwise 0,  to Future_change value of the current quarter\n",
    "data[['EPS','change_in_EPS','Future_change']].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine data \n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinity with nan\n",
    "data = data.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows where change_in_EPS is nan: they are no use to us \n",
    "data = data.dropna(subset = ['change_in_EPS', 'Future_change'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We no longer need these columns\n",
    "data = data.drop(columns = ['EPS','change_in_EPS','binary_change'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine missing data\n",
    "missing_column_data = 100*(data.isnull().sum() / data.shape[0]).round(3)\n",
    "print('Percent of missing values per column:\\n', missing_column_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 10 columns that have more than 30% of data missing\n",
    "columns_to_drop = missing_column_data[missing_column_data > 30]\n",
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of columns dropped, 10 \n",
    "data = data.drop(columns = list(columns_to_drop.index))\n",
    "print( f'New Dataframe shape : {data.shape}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data:\n",
    "- Handle remaining missing values\n",
    "- Minimize influence of outliers by performing Winsorization\n",
    "- Standardize data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle remaining missing data by replacing NaN by mean of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep in mind that this is a naive way to handle missing values. \n",
    "# This method can cause data leakage and does not factor the covariance between features.\n",
    "# For more robust methods,take a look at MICE and KNN\n",
    "\n",
    "for col in data.columns:\n",
    "    data[col].fillna(data[col].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_column_data = 100*(data.isnull().sum()/ data.shape[0]).round(3)\n",
    "print('Percent of missing values per column:\\n',missing_column_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to split our data into train and test. \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Independent values/features\n",
    "X = data.iloc[:,:-1].values\n",
    "# Dependent values\n",
    "y = data.iloc[:,-1].values\n",
    "\n",
    "# Create test and train data sets, split data randomly into 20% test and 80% train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winsorization transforms data by limiting extreme values, typically by setting all outliers to a specified percentile of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mstats\n",
    "# Winsorize top 1% and bottom 1% of points. \n",
    "\n",
    "# Apply on X_train and X_test separately\n",
    "X_train = mstats.winsorize(X_train, limits = [0.01, 0.01])\n",
    "X_test = mstats.winsorize(X_test, limits = [0.01, 0.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the data\n",
    "\n",
    "$$z=(x-mean) /  Standard Deviation$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
    "\n",
    "For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines (SVM) or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features by removing the mean and scaling to unit variance.\n",
    "\n",
    "# IMPORTANT: During testing, it is important to construct the test feature vectors using the means and standard deviations saved from\n",
    "# the training data, rather than computing it from the test data. You must scale your test inputs using the saved means\n",
    "# and standard deviations, prior to sending them to your SVM library for classification.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit to training data and then transform it\n",
    "X_train = sc.fit_transform(X_train)\n",
    "# Perform standardization on testing data using mu and sigma from training data\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source: scikit-learn](https://scikit-learn.org/stable/modules/svm.html) <br>\n",
    "\n",
    "### SVM\n",
    "\n",
    "**Advantages:**\n",
    "* Effective in high dimensional spaces.\n",
    "* Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "**Disadvantages:**\n",
    "* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "* It also doesn’t perform very well when the data set has more noise i.e. when target classes are overlapping.\n",
    "* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/svm.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Classification(C)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize svm, rbf is a default kernel\n",
    "classifier_rbf = SVC(C = 1, kernel = 'rbf', gamma = 'auto', random_state = 0)\n",
    "\n",
    "# Fit the model on training data\n",
    "classifier_rbf.fit(X_train, y_train)\n",
    "\n",
    "# Make a prediction on testing data\n",
    "y_pred_rbf = classifier_rbf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "ac_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "print('Accuracy with RBF: {:.2f}'.format(ac_rbf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision and recall\n",
    "from sklearn.metrics import classification_report\n",
    "result = classification_report(y_test, y_pred_rbf)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters:\n",
    "- Kernel - transforms the data into a required form(dimension) so the data can be separated. RBF is useful for non-linear hyperplane in higher dimensions\n",
    "  and computes the separation line in the higher dimension. In some of the applications, it is suggested to use a more complex kernel to separate the classes that are curved or nonlinear.\n",
    "- Regularization, C - penalty parameter, which represents misclassification or error. It tells the SVM optimization how much error is bearable. Small C results in a small-margin hyperplane while large C in large margin hyperplane.\n",
    "- Gamma - defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors. Higher values of gamma will exactly fit the training dataset, which can causes over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default C = 1, let's change kernel to linear\n",
    "classifier_lin = SVC(C = 1, kernel = 'linear',gamma = 'auto',random_state=0)\n",
    "\n",
    "# Fit the model on training data\n",
    "classifier_lin.fit(X_train, y_train)\n",
    "\n",
    "# Make a prediction on testing data\n",
    "y_pred_lin = classifier_lin.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "ac_lin = accuracy_score(y_test, y_pred_lin)\n",
    "print('Accuracy with Linear: {:.2f}'.format(ac_lin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we speed up our SVM algorithm ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis (PCA)\n",
    "- Common way to speed up machine learning algorithms\n",
    "- Large number of features in the dataset can affect both the training times and accuracy of the model\n",
    "- PCA is a statistical technique that reduces number of features to those that capture maximum information about the dataset\n",
    "- Features are selected on the basis of their variance - higher the variance, more information that component conveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# keep 95% of variance\n",
    "pca = PCA(0.95)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components that explain 95% of variance in our dataset\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "# 27 features explain 95% of variance, down from original 40\n",
    "len(explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Able to achieve similar accuracy but with only 27 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(C = 1, kernel='rbf',gamma = 'auto',random_state=0)\n",
    "\n",
    "classifier.fit(X_train_pca, y_train)\n",
    "y_pred = classifier.predict(X_test_pca)\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.2f}'.format(ac))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Additional Resources</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Python Libraries</h4>\n",
    "\n",
    "Scikit train_test_split:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "Scikit SVM:\n",
    "https://scikit-learn.org/stable/modules/svm.html\n",
    "    \n",
    "PCA:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "Missing values imputation\n",
    "https://scikit-learn.org/stable/modules/impute.html\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (sandboxed)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
